"""
SmolVLA æœ¬åœ°æ¨ç†æµ‹è¯•è„šæœ¬ï¼ˆæœ€ç»ˆä¿®å¤ç‰ˆï¼‰
å…³é”®ï¼šéœ€è¦ batch ç»´åº¦ï¼
"""

import torch
import numpy as np
from PIL import Image
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
from torchvision import transforms
import time

def test_model_loading():
    """æµ‹è¯•1ï¼šæ¨¡å‹åŠ è½½"""
    print("=" * 60)
    print("æµ‹è¯•1ï¼šåŠ è½½SmolVLAé¢„è®­ç»ƒæ¨¡å‹")
    print("=" * 60)
    
    try:
        print("æ­£åœ¨ä¸‹è½½æ¨¡å‹...")
        policy = SmolVLAPolicy.from_pretrained("lerobot/smolvla_base")
        
        # ç§»åˆ°GPUå¹¶è½¬æ¢ç²¾åº¦
        policy = policy.to("cuda")
        policy = policy.half()
        policy.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹è®¾å¤‡: {next(policy.parameters()).device}")
        print(f"   æ¨¡å‹ç²¾åº¦: {next(policy.parameters()).dtype}")
        
        # æ‰“å°æ¨¡å‹æœŸæœ›çš„è¾“å…¥
        print(f"   æœŸæœ›çš„ç›¸æœº: {list(policy.config.input_features.keys())}")
        
        # ç»Ÿè®¡å‚æ•°é‡
        total_params = sum(p.numel() for p in policy.parameters())
        print(f"   æ€»å‚æ•°é‡: {total_params / 1e6:.2f}M")
        
        return policy
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_dummy_inference(policy):
    """æµ‹è¯•2ï¼šè™šæ‹Ÿæ•°æ®æ¨ç†ï¼ˆæ­£ç¡®çš„ç»´åº¦ï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2ï¼šè™šæ‹Ÿæ•°æ®æ¨ç†")
    print("=" * 60)
    
    try:
        # å…³é”®ï¼šéœ€è¦ (batch, channel, height, width) æ ¼å¼ï¼
        # batch=1, channel=3, height=256, width=256
        dummy_image = torch.rand(1, 3, 256, 256).cuda().half()  # â† æ³¨æ„è¿™é‡Œæ˜¯ 4 ç»´ï¼
        dummy_state = torch.randn(1, 7).cuda().half()  # â† state ä¹Ÿè¦ batch ç»´åº¦
        
        # ä½¿ç”¨æ¨¡å‹æœŸæœ›çš„é”®å
        observation = {
            "observation.images.camera1": dummy_image,
            "observation.state": dummy_state,
        }
        
        print(f"   å›¾åƒå½¢çŠ¶: {dummy_image.shape} (batch, c, h, w)")
        print(f"   çŠ¶æ€ç»´åº¦: {dummy_state.shape} (batch, state_dim)")
        
        # æ¨ç†
        print("   å¼€å§‹æ¨ç†...")
        start_time = time.time()
        
        with torch.no_grad():
            actions = policy.select_action(observation)
        
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸï¼")
        print(f"   æ¨ç†æ—¶é—´: {inference_time * 1000:.2f}ms")
        print(f"   è¾“å‡ºå½¢çŠ¶: {actions.shape}")
        print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min().item():.3f}, {actions.max().item():.3f}]")
        
        # æ£€æŸ¥è¾“å‡º
        if torch.isnan(actions).any():
            print("âš ï¸  è­¦å‘Š: è¾“å‡ºåŒ…å«NaN")
        if torch.isinf(actions).any():
            print("âš ï¸  è­¦å‘Š: è¾“å‡ºåŒ…å«Inf")
            
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_pil_to_tensor(policy):
    """æµ‹è¯•3ï¼šä»PILå›¾åƒæ¨ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3ï¼šPILå›¾åƒè½¬æ¢æ¨ç†")
    print("=" * 60)
    
    try:
        # åˆ›å»ºPILå›¾åƒ
        pil_image = Image.fromarray(
            np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        )
        
        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦
        transform = transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.ToTensor(),  # è¾“å‡º (3, 256, 256)
        ])
        
        image_tensor = transform(pil_image)  # (3, 256, 256)
        image_tensor = image_tensor.unsqueeze(0)  # æ·»åŠ batchç»´åº¦ â†’ (1, 3, 256, 256)
        image_tensor = image_tensor.cuda().half()
        
        dummy_state = torch.randn(1, 7).cuda().half()
        
        observation = {
            "observation.images.camera1": image_tensor,
            "observation.state": dummy_state,
        }
        
        print(f"   PILå›¾åƒ: {pil_image.size}")
        print(f"   è½¬æ¢å: {image_tensor.shape}")
        
        # æ¨ç†
        start_time = time.time()
        
        with torch.no_grad():
            actions = policy.select_action(observation)
        
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸ")
        print(f"   æ¨ç†æ—¶é—´: {inference_time * 1000:.2f}ms")
        
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_batch_inference(policy):
    """æµ‹è¯•4ï¼šæ‰¹é‡æ¨ç†"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•4ï¼šæ‰¹é‡æ¨ç†ï¼ˆbatch_size=4ï¼‰")
    print("=" * 60)
    
    try:
        batch_size = 4
        
        # åˆ›å»ºæ‰¹é‡æ•°æ®
        images = torch.rand(batch_size, 3, 256, 256).cuda().half()
        states = torch.randn(batch_size, 7).cuda().half()
        
        observation = {
            "observation.images.camera1": images,
            "observation.state": states,
        }
        
        print(f"   Batch size: {batch_size}")
        print(f"   å›¾åƒå½¢çŠ¶: {images.shape}")
        print(f"   çŠ¶æ€å½¢çŠ¶: {states.shape}")
        
        # æ¨ç†
        start_time = time.time()
        
        with torch.no_grad():
            actions = policy.select_action(observation)
        
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸ")
        print(f"   æ€»æ—¶é—´: {inference_time * 1000:.2f}ms")
        print(f"   å¹³å‡æ—¶é—´: {inference_time * 1000 / batch_size:.2f}ms/sample")
        print(f"   è¾“å‡ºå½¢çŠ¶: {actions.shape}")
        
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_speed(policy, num_iterations=20):
    """æµ‹è¯•5ï¼šæ¨ç†é€Ÿåº¦"""
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•5ï¼šæ¨ç†é€Ÿåº¦ï¼ˆ{num_iterations}æ¬¡ï¼‰")
    print("=" * 60)
    
    times = []
    
    for i in range(num_iterations):
        # åˆ›å»ºéšæœºè¾“å…¥
        image = torch.rand(1, 3, 256, 256).cuda().half()
        state = torch.randn(1, 7).cuda().half()
        
        observation = {
            "observation.images.camera1": image,
            "observation.state": state,
        }
        
        # æ¨ç†
        start = time.time()
        with torch.no_grad():
            _ = policy.select_action(observation)
        times.append(time.time() - start)
        
        if (i + 1) % 5 == 0:
            print(f"   è¿›åº¦: {i+1}/{num_iterations}")
    
    times = np.array(times) * 1000  # è½¬æ¢ä¸ºms
    
    print(f"\nâœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ")
    print(f"   å¹³å‡: {times.mean():.2f}ms")
    print(f"   ä¸­ä½æ•°: {np.median(times):.2f}ms")
    print(f"   æœ€å°: {times.min():.2f}ms")
    print(f"   æœ€å¤§: {times.max():.2f}ms")
    print(f"   æ ‡å‡†å·®: {times.std():.2f}ms")
    print(f"   æ¨ç†é¢‘ç‡: {1000 / times.mean():.2f} Hz")
    
    return times

def test_gpu_memory():
    """æµ‹è¯•6ï¼šæ˜¾å­˜ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•6ï¼šGPUæ˜¾å­˜ä½¿ç”¨")
    print("=" * 60)
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"   å·²åˆ†é…: {allocated:.2f} GB")
        print(f"   å·²é¢„ç•™: {reserved:.2f} GB")
        print(f"   æ€»æ˜¾å­˜: {total:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {(allocated / total) * 100:.1f}%")
        
        if allocated < 8.0:
            print("âœ… æ˜¾å­˜ä½¿ç”¨æ­£å¸¸ (<8GB)")
        elif allocated < 12.0:
            print("âš ï¸  æ˜¾å­˜ä½¿ç”¨åé«˜ (8-12GB)")
        else:
            print("âŒ æ˜¾å­˜ä½¿ç”¨è¿‡é«˜ (>12GB)")

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸš€" * 30)
    print("SmolVLA æœ¬åœ°æ¨ç†å®Œæ•´æµ‹è¯•ï¼ˆæœ€ç»ˆç‰ˆï¼‰")
    print("ğŸš€" * 30 + "\n")
    
    try:
        # æµ‹è¯•1: åŠ è½½æ¨¡å‹
        policy = test_model_loading()
        
        # æµ‹è¯•2: åŸºç¡€æ¨ç†
        test_dummy_inference(policy)
        
        # æµ‹è¯•3: PILå›¾åƒ
        test_pil_to_tensor(policy)
        
        # æµ‹è¯•4: æ‰¹é‡æ¨ç†
        test_batch_inference(policy)
        
        # æµ‹è¯•5: é€Ÿåº¦æµ‹è¯•
        test_speed(policy, num_iterations=20)
        
        # æµ‹è¯•6: æ˜¾å­˜
        test_gpu_memory()
        
        print("\n" + "=" * 60)
        print("âœ…âœ…âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼SmolVLAæœ¬åœ°æ¨ç†ç¨³å®šè¿è¡Œ âœ…âœ…âœ…")
        print("=" * 60)
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. æŸ¥çœ‹æ€§èƒ½æ˜¯å¦è¾¾æ ‡ï¼ˆæ¨ç†<150msï¼Œæ˜¾å­˜<8GBï¼‰")
        print("2. å¦‚æœè¾¾æ ‡ï¼Œå¯ä»¥è¿›å…¥æ¨¡æ‹Ÿç¯å¢ƒæµ‹è¯•é˜¶æ®µ")
        print("3. å‚è€ƒæ–‡æ¡£ï¼š01-smolvlaæœ¬åœ°ç¨³å®šå¤ç°.md")
        
    except Exception as e:
        print("\n" + "=" * 60)
        print(f"âŒ æµ‹è¯•å¤±è´¥")
        print("=" * 60)
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
