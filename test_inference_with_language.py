"""
SmolVLA å®Œæ•´æ¨ç†æµ‹è¯•ï¼ˆåŒ…å«è¯­è¨€æŒ‡ä»¤ï¼‰
"""

import torch
import numpy as np
from PIL import Image
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
from lerobot.policies.smolvla.processor_smolvla import SmolVLAProcessor
from torchvision import transforms
import time

def test_model_and_processor_loading():
    """æµ‹è¯•1ï¼šåŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨"""
    print("=" * 60)
    print("æµ‹è¯•1ï¼šåŠ è½½SmolVLAæ¨¡å‹å’Œå¤„ç†å™¨")
    print("=" * 60)
    
    try:
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        policy = SmolVLAPolicy.from_pretrained("lerobot/smolvla_base")
        policy = policy.to("cuda").half().eval()
        
        print("æ­£åœ¨åŠ è½½å¤„ç†å™¨...")
        processor = SmolVLAProcessor.from_pretrained("lerobot/smolvla_base")
        
        print("âœ… åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹å‚æ•°: {sum(p.numel() for p in policy.parameters()) / 1e6:.2f}M")
        print(f"   æ¨¡å‹è®¾å¤‡: {next(policy.parameters()).device}")
        print(f"   æ¨¡å‹ç²¾åº¦: {next(policy.parameters()).dtype}")
        
        return policy, processor
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_with_processor(policy, processor):
    """æµ‹è¯•2ï¼šä½¿ç”¨processorå¤„ç†è¾“å…¥"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•2ï¼šä½¿ç”¨Processorå¤„ç†è¾“å…¥")
    print("=" * 60)
    
    try:
        # åˆ›å»ºPILå›¾åƒ
        pil_image = Image.fromarray(
            np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)
        )
        
        # è¯­è¨€æŒ‡ä»¤
        instruction = "pick up the red cube"
        
        print(f"   å›¾åƒå¤§å°: {pil_image.size}")
        print(f"   æŒ‡ä»¤: '{instruction}'")
        
        # ä½¿ç”¨processorå¤„ç†
        print("   å¤„ç†è¾“å…¥...")
        processed = processor(
            images=[pil_image],
            text=[instruction]
        )
        
        print(f"   å¤„ç†åçš„é”®: {processed.keys()}")
        
        # ç§»åˆ°GPUå¹¶è½¬æ¢ç±»å‹
        observation = {}
        for key, value in processed.items():
            if isinstance(value, torch.Tensor):
                observation[key] = value.cuda().half()
            else:
                observation[key] = value
        
        # æ·»åŠ çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰
        observation["observation.state"] = torch.randn(1, 7).cuda().half()
        
        print("   å¼€å§‹æ¨ç†...")
        start_time = time.time()
        
        with torch.no_grad():
            actions = policy.select_action(observation)
        
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸï¼")
        print(f"   æ¨ç†æ—¶é—´: {inference_time * 1000:.2f}ms")
        print(f"   è¾“å‡ºå½¢çŠ¶: {actions.shape}")
        print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min().item():.3f}, {actions.max().item():.3f}]")
        
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_manual_preparation(policy, processor):
    """æµ‹è¯•3ï¼šæ‰‹åŠ¨å‡†å¤‡è¾“å…¥ï¼ˆä¸ä½¿ç”¨processorï¼‰"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•3ï¼šæ‰‹åŠ¨å‡†å¤‡è¾“å…¥")
    print("=" * 60)
    
    try:
        # åˆ›å»ºå›¾åƒtensor
        image = torch.rand(1, 3, 256, 256).cuda().half()
        state = torch.randn(1, 7).cuda().half()
        
        # æ‰‹åŠ¨åˆ›å»ºlanguage tokensï¼ˆä½¿ç”¨tokenizerï¼‰
        instruction = "move to target position"
        
        # ä½¿ç”¨processorçš„tokenizer
        tokens = processor.tokenizer(
            instruction,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        # å‡†å¤‡observation
        observation = {
            "observation.images.camera1": image,
            "observation.state": state,
            "observation.language.tokens": tokens["input_ids"].cuda(),
        }
        
        print(f"   å›¾åƒå½¢çŠ¶: {image.shape}")
        print(f"   çŠ¶æ€å½¢çŠ¶: {state.shape}")
        print(f"   Tokenå½¢çŠ¶: {tokens['input_ids'].shape}")
        print(f"   æŒ‡ä»¤: '{instruction}'")
        
        # æ¨ç†
        start_time = time.time()
        
        with torch.no_grad():
            actions = policy.select_action(observation)
        
        inference_time = time.time() - start_time
        
        print(f"âœ… æ¨ç†æˆåŠŸ")
        print(f"   æ¨ç†æ—¶é—´: {inference_time * 1000:.2f}ms")
        
        return actions
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise

def test_speed(policy, processor, num_iterations=10):
    """æµ‹è¯•4ï¼šæ¨ç†é€Ÿåº¦"""
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•4ï¼šæ¨ç†é€Ÿåº¦ï¼ˆ{num_iterations}æ¬¡ï¼‰")
    print("=" * 60)
    
    times = []
    instructions = [
        "pick up the red cube",
        "place object in box",
        "move to target position",
        "grasp the bottle",
        "push the button"
    ]
    
    for i in range(num_iterations):
        # åˆ›å»ºè¾“å…¥
        image = torch.rand(1, 3, 256, 256).cuda().half()
        state = torch.randn(1, 7).cuda().half()
        instruction = instructions[i % len(instructions)]
        
        tokens = processor.tokenizer(
            instruction,
            return_tensors="pt",
            padding=True,
            truncation=True
        )
        
        observation = {
            "observation.images.camera1": image,
            "observation.state": state,
            "observation.language.tokens": tokens["input_ids"].cuda(),
        }
        
        # æ¨ç†
        start = time.time()
        with torch.no_grad():
            _ = policy.select_action(observation)
        times.append(time.time() - start)
        
        if (i + 1) % 5 == 0:
            print(f"   è¿›åº¦: {i+1}/{num_iterations}")
    
    times = np.array(times) * 1000
    
    print(f"\nâœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ")
    print(f"   å¹³å‡: {times.mean():.2f}ms")
    print(f"   ä¸­ä½æ•°: {np.median(times):.2f}ms")
    print(f"   æœ€å°: {times.min():.2f}ms")
    print(f"   æœ€å¤§: {times.max():.2f}ms")
    print(f"   æ¨ç†é¢‘ç‡: {1000 / times.mean():.2f} Hz")
    
    # æ€§èƒ½è¯„ä¼°
    if times.mean() < 100:
        print("   æ€§èƒ½ç­‰çº§: â­â­â­â­â­ ä¼˜ç§€ (A)")
    elif times.mean() < 150:
        print("   æ€§èƒ½ç­‰çº§: â­â­â­â­ è‰¯å¥½ (B)")
    elif times.mean() < 200:
        print("   æ€§èƒ½ç­‰çº§: â­â­â­ åŠæ ¼ (C)")
    else:
        print("   æ€§èƒ½ç­‰çº§: â­â­ éœ€è¦ä¼˜åŒ– (D)")
    
    return times

def test_gpu_memory():
    """æµ‹è¯•5ï¼šæ˜¾å­˜ä½¿ç”¨"""
    print("\n" + "=" * 60)
    print("æµ‹è¯•5ï¼šGPUæ˜¾å­˜ä½¿ç”¨")
    print("=" * 60)
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        print(f"   å·²åˆ†é…: {allocated:.2f} GB")
        print(f"   å·²é¢„ç•™: {reserved:.2f} GB")
        print(f"   æ€»æ˜¾å­˜: {total:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {(allocated / total) * 100:.1f}%")
        
        if allocated < 8.0:
            print("   ç­‰çº§: âœ… ä¼˜ç§€ (<8GB)")
        elif allocated < 12.0:
            print("   ç­‰çº§: âš ï¸  è‰¯å¥½ (8-12GB)")
        else:
            print("   ç­‰çº§: âŒ åé«˜ (>12GB)")

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("\n" + "ğŸ¯" * 30)
    print("SmolVLA å®Œæ•´æ¨ç†æµ‹è¯•ï¼ˆåŒ…å«è¯­è¨€æŒ‡ä»¤ï¼‰")
    print("ğŸ¯" * 30 + "\n")
    
    try:
        # æµ‹è¯•1: åŠ è½½
        policy, processor = test_model_and_processor_loading()
        
        # æµ‹è¯•2: ä½¿ç”¨processor
        test_with_processor(policy, processor)
        
        # æµ‹è¯•3: æ‰‹åŠ¨å‡†å¤‡
        test_manual_preparation(policy, processor)
        
        # æµ‹è¯•4: é€Ÿåº¦
        test_speed(policy, processor, num_iterations=10)
        
        # æµ‹è¯•5: æ˜¾å­˜
        test_gpu_memory()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“Š æ€§èƒ½æ€»ç»“ï¼š")
        print("- å¦‚æœæ¨ç†æ—¶é—´ < 150ms ä¸”æ˜¾å­˜ < 8GB â†’ æ€§èƒ½è¾¾æ ‡ âœ…")
        print("- å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼šæ¨¡æ‹Ÿç¯å¢ƒæµ‹è¯•")
        print("\nğŸ“š ä¸‹ä¸€æ­¥ï¼š")
        print("1. æŸ¥çœ‹æ–‡æ¡£ï¼š01-smolvlaæœ¬åœ°ç¨³å®šå¤ç°.md ç¬¬å››ç« ")
        print("2. è¿è¡Œæ¨¡æ‹Ÿç¯å¢ƒæµ‹è¯•")
        print("3. å¡«å†™é¡¹ç›®è¿›åº¦æ¸…å•")
        
    except Exception as e:
        print("\n" + "=" * 60)
        print("âŒ æµ‹è¯•å¤±è´¥")
        print("=" * 60)
        import traceback
        traceback.print_exc()
        print("\nğŸ’¡ å¸¸è§é—®é¢˜ï¼š")
        print("1. æ¨¡å‹ä¸‹è½½å¤±è´¥ â†’ ä½¿ç”¨ HF é•œåƒ")
        print("2. CUDAä¸å¯ç”¨ â†’ æ£€æŸ¥ nvidia-smi")
        print("3. å¯¼å…¥å¤±è´¥ â†’ é‡æ–° pip install -e .")

if __name__ == "__main__":
    main()
