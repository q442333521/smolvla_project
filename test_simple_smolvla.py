"""
SmolVLA æœ€ç®€å•çš„æ¨ç†æµ‹è¯•
æ‰‹åŠ¨å‡†å¤‡æ‰€æœ‰å¿…éœ€çš„è¾“å…¥
"""

import torch
import numpy as np
from PIL import Image
from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
from transformers import AutoTokenizer
import time

print("ğŸš€ å¼€å§‹ SmolVLA ç®€å•æ¨ç†æµ‹è¯•\n")

# æ­¥éª¤1ï¼šåŠ è½½æ¨¡å‹
print("=" * 60)
print("æ­¥éª¤1ï¼šåŠ è½½æ¨¡å‹")
print("=" * 60)

policy = SmolVLAPolicy.from_pretrained("lerobot/smolvla_base")
policy = policy.to("cuda").half().eval()

print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
print(f"   å‚æ•°é‡: {sum(p.numel() for p in policy.parameters()) / 1e6:.2f}M")
print(f"   è®¾å¤‡: {next(policy.parameters()).device}")

# æ­¥éª¤2ï¼šåŠ è½½tokenizer
print("\n" + "=" * 60)
print("æ­¥éª¤2ï¼šåŠ è½½Tokenizer")
print("=" * 60)

# ä»configä¸­è·å–VLMæ¨¡å‹åç§°
vlm_model_name = policy.config.vlm_model_name
print(f"   VLMæ¨¡å‹: {vlm_model_name}")

tokenizer = AutoTokenizer.from_pretrained(vlm_model_name)
print("âœ… TokenizeråŠ è½½æˆåŠŸ")

# æ­¥éª¤3ï¼šå‡†å¤‡è¾“å…¥æ•°æ®
print("\n" + "=" * 60)
print("æ­¥éª¤3ï¼šå‡†å¤‡è¾“å…¥æ•°æ®")
print("=" * 60)

# å›¾åƒï¼š(batch, channel, height, width)
image = torch.rand(1, 3, 256, 256).cuda().half()
print(f"   å›¾åƒå½¢çŠ¶: {image.shape}")

# çŠ¶æ€ï¼š(batch, state_dim)
state = torch.randn(1, 7).cuda().half()
print(f"   çŠ¶æ€å½¢çŠ¶: {state.shape}")

# è¯­è¨€æŒ‡ä»¤
instruction = "pick up the red cube"
print(f"   æŒ‡ä»¤: '{instruction}'")

# TokenåŒ–æŒ‡ä»¤
tokens = tokenizer(
    instruction,
    return_tensors="pt",
    padding="max_length",
    truncation=True,
    max_length=77  # é»˜è®¤é•¿åº¦
)
print(f"   Tokenå½¢çŠ¶: {tokens['input_ids'].shape}")

# å‡†å¤‡observationå­—å…¸
observation = {
    "observation.images.camera1": image,
    "observation.state": state,
    "observation.language.tokens": tokens["input_ids"].cuda(),
}

print("âœ… è¾“å…¥æ•°æ®å‡†å¤‡å®Œæˆ")

# æ­¥éª¤4ï¼šæ¨ç†
print("\n" + "=" * 60)
print("æ­¥éª¤4ï¼šæ‰§è¡Œæ¨ç†")
print("=" * 60)

start_time = time.time()

with torch.no_grad():
    actions = policy.select_action(observation)

inference_time = time.time() - start_time

print(f"âœ… æ¨ç†æˆåŠŸï¼")
print(f"   æ¨ç†æ—¶é—´: {inference_time * 1000:.2f}ms")
print(f"   è¾“å‡ºå½¢çŠ¶: {actions.shape}")
print(f"   åŠ¨ä½œèŒƒå›´: [{actions.min().item():.3f}, {actions.max().item():.3f}]")

# æ­¥éª¤5ï¼šå¤šæ¬¡æ¨ç†æµ‹è¯•é€Ÿåº¦
print("\n" + "=" * 60)
print("æ­¥éª¤5ï¼šé€Ÿåº¦æµ‹è¯•ï¼ˆ10æ¬¡ï¼‰")
print("=" * 60)

times = []
for i in range(10):
    image = torch.rand(1, 3, 256, 256).cuda().half()
    state = torch.randn(1, 7).cuda().half()
    
    observation = {
        "observation.images.camera1": image,
        "observation.state": state,
        "observation.language.tokens": tokens["input_ids"].cuda(),
    }
    
    start = time.time()
    with torch.no_grad():
        _ = policy.select_action(observation)
    times.append(time.time() - start)

times = np.array(times) * 1000  # è½¬ä¸ºms

print(f"âœ… é€Ÿåº¦æµ‹è¯•å®Œæˆ")
print(f"   å¹³å‡: {times.mean():.2f}ms")
print(f"   æœ€å°: {times.min():.2f}ms")
print(f"   æœ€å¤§: {times.max():.2f}ms")
print(f"   æ¨ç†é¢‘ç‡: {1000 / times.mean():.2f} Hz")

# æ€§èƒ½è¯„çº§
if times.mean() < 100:
    grade = "â­â­â­â­â­ ä¼˜ç§€ (A)"
elif times.mean() < 150:
    grade = "â­â­â­â­ è‰¯å¥½ (B)"
elif times.mean() < 200:
    grade = "â­â­â­ åŠæ ¼ (C)"
else:
    grade = "â­â­ éœ€ä¼˜åŒ– (D)"
print(f"   æ€§èƒ½ç­‰çº§: {grade}")

# æ­¥éª¤6ï¼šæ˜¾å­˜æ£€æŸ¥
print("\n" + "=" * 60)
print("æ­¥éª¤6ï¼šæ˜¾å­˜æ£€æŸ¥")
print("=" * 60)

allocated = torch.cuda.memory_allocated() / 1024**3
reserved = torch.cuda.memory_reserved() / 1024**3
total = torch.cuda.get_device_properties(0).total_memory / 1024**3

print(f"   å·²åˆ†é…: {allocated:.2f} GB")
print(f"   å·²é¢„ç•™: {reserved:.2f} GB")
print(f"   æ€»æ˜¾å­˜: {total:.2f} GB")
print(f"   ä½¿ç”¨ç‡: {(allocated / total) * 100:.1f}%")

if allocated < 8.0:
    print("   ç­‰çº§: âœ… ä¼˜ç§€ (<8GB)")
elif allocated < 12.0:
    print("   ç­‰çº§: âš ï¸  è‰¯å¥½ (8-12GB)")
else:
    print("   ç­‰çº§: âŒ åé«˜ (>12GB)")

# æœ€ç»ˆæ€»ç»“
print("\n" + "ğŸ‰" * 30)
print("æµ‹è¯•å®Œæˆæ€»ç»“")
print("ğŸ‰" * 30)

print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡ï¼š")
print(f"   âœ… æ¨ç†é€Ÿåº¦: {times.mean():.2f}ms ({grade})")
print(f"   âœ… æ˜¾å­˜å ç”¨: {allocated:.2f}GB")
print(f"   âœ… æ¨ç†é¢‘ç‡: {1000/times.mean():.2f}Hz")

print(f"\nâœ… éªŒæ”¶æ ‡å‡†ï¼š")
è¾¾æ ‡é¡¹ = 0
æ€»é¡¹ = 2

if times.mean() < 150:
    print(f"   âœ… æ¨ç†æ—¶é—´ < 150ms")
    è¾¾æ ‡é¡¹ += 1
else:
    print(f"   âŒ æ¨ç†æ—¶é—´ {times.mean():.2f}ms (ç›®æ ‡<150ms)")

if allocated < 8.0:
    print(f"   âœ… æ˜¾å­˜å ç”¨ < 8GB")
    è¾¾æ ‡é¡¹ += 1
else:
    print(f"   âš ï¸  æ˜¾å­˜å ç”¨ {allocated:.2f}GB (ç›®æ ‡<8GB)")

print(f"\nğŸ¯ è¾¾æ ‡ç‡: {è¾¾æ ‡é¡¹}/{æ€»é¡¹} ({è¾¾æ ‡é¡¹/æ€»é¡¹*100:.0f}%)")

if è¾¾æ ‡é¡¹ == æ€»é¡¹:
    print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰æŒ‡æ ‡è¾¾æ ‡ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼")
    print("ğŸ“š ä¸‹ä¸€æ­¥ï¼š")
    print("   1. è¿è¡Œæ¨¡æ‹Ÿç¯å¢ƒæµ‹è¯•")
    print("   2. å¡«å†™é¡¹ç›®è¿›åº¦æ¸…å•")
    print("   3. å‡†å¤‡ROS2é›†æˆ")
else:
    print("\nâš ï¸  éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œä½†ä»å¯ç»§ç»­")
    print("ğŸ’¡ ä¼˜åŒ–å»ºè®®ï¼š")
    if times.mean() >= 150:
        print("   - ä½¿ç”¨torch.compile()åŠ é€Ÿ")
        print("   - å‡å°å›¾åƒåˆ†è¾¨ç‡")
    if allocated >= 8.0:
        print("   - ä½¿ç”¨æ›´ä½ç²¾åº¦ï¼ˆINT8ï¼‰")
        print("   - å‡å°batch size")
